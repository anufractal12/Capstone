{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "320008e8-749e-4894-bbea-812ebbfdcf59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls('/mnt/anuaqstorageacc/Air_Quality_Target/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e72cf55-b676-421d-ab85-e20752b7c4ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Listing all the files and sorting them based on latest timestamp\n",
    " \n",
    "files = dbutils.fs.ls('/mnt/anuaqstorageacc/Air_Quality_Target/')\n",
    "latest_file = sorted(files, key=lambda x: x.modificationTime, reverse=True)[0]\n",
    " \n",
    "print(f\"Latest File: {latest_file.path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09c7296b-f7cc-4498-ba98-ee5c5037bab5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(latest_file.path)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d938b5bc-726a-4064-b91c-1a7af4b7f314",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "states_all = [\"Andhra Pradesh\",\"Arunachal Pradesh \",\"Assam\",\"Bihar\",\"Chhattisgarh\",\"Delhi\",\"Goa\",\"Gujarat\",\"Haryana\",\"Himachal Pradesh\",\"Jharkhand\",\"Karnataka\",\"Kerala\",\"Madhya Pradesh\",\"Maharashtra\",\"Manipur\",\"Meghalaya\",\"Mizoram\",\"Nagaland\",\"Odisha\",\"Punjab\",\"Rajasthan\",\"Sikkim\",\"Tamil Nadu\",\"Telangana\",\"Tripura\",\"Uttar Pradesh\",\"Uttarakhand\",\"West Bengal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ddddd39-3df6-45f2-9a80-2c0de4bf6d0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# State value should be from all valid states of India\n",
    " \n",
    "alarm_states = df.filter(df.state.isin(states_all) == False)\n",
    " \n",
    "if alarm_states.count() > 0:\n",
    "  print(f\"{alarm_states.count()} rows have incorrect state values\")\n",
    "else:\n",
    "  print(\"All States are Correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39571fa8-a9c0-4c97-9c4b-3d6a46856d45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Values of latitude and longitude should be within valid maximum and minimum i.e. -90 to 90 and -180 to 180\n",
    " \n",
    "df_check = df.filter((df.latitude.cast(\"Double\") < -90) | (df.latitude.cast(\"Double\") > 90) | (df.longitude.cast(\"Double\") < -180) | (df.longitude.cast(\"Double\") > 180))\n",
    " \n",
    "if df_check.count() > 0:\n",
    "  print(f\"{df_check.count} rows have incorrect latitude or longitude values\")\n",
    "else:\n",
    "  print(\"All latitude and longitude values are correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05c79e14-f8a1-4628-83f7-19c0a4addd9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# OZONE value should be less than 100\n",
    "# CO value should be less than 7\n",
    "# SO2 value should be less than 40\n",
    " \n",
    "# Cleaning Pollutant_Avg\n",
    "df_pollutant = df.withColumn(\"pollutant_avg\", F.when(F.col('pollutant_avg') == 'NA', 0).otherwise(F.col('pollutant_avg')))\n",
    " \n",
    "ozone_range = df_pollutant.filter((df_pollutant.pollutant_id == 'OZONE') & (df_pollutant.pollutant_avg.cast(\"Double\") > 100))\n",
    "co_range = df_pollutant.filter((df_pollutant.pollutant_id == 'CO') & (df_pollutant.pollutant_avg.cast(\"Double\") > 7))\n",
    "so2_range = df_pollutant.filter((df_pollutant.pollutant_id == 'SO2') & (df_pollutant.pollutant_avg.cast(\"Double\") > 40))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "110cc7b3-b094-4144-b243-57a74ed44bf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Writing the data into the Bronze layer\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"faampn6.anuradha.Air_Quality_Bronze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4634563b-3e2a-4b92-b0e6-f71d45de240d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(\"faampn6.anuradha.Air_Quality_Bronze\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a47302a-efa2-422b-9421-5428630f8729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To keep the Capital Cities of each state we need to map the key-value pair\n",
    " \n",
    "capitals = {\n",
    "  \"Andhra Pradesh\": \"Amaravati\",\n",
    "  \"Arunachal Pradesh\": \"Itanagar\",\n",
    "  \"Assam\": \"Dispur\",\n",
    "  \"Bihar\": \"Patna\",\n",
    "  \"Chhattisgarh\": \"Raipur\",\n",
    "  \"Delhi\": \"New Delhi\",\n",
    "  \"Goa\": \"Panaji\",\n",
    "  \"Gujarat\": \"Gandhinagar\",\n",
    "  \"Haryana\": \"Chandigarh\",\n",
    "  \"Himachal Pradesh\": \"Shimla\",\n",
    "  \"Jharkhand\": \"Ranchi\",\n",
    "  \"Karnataka\": \"Bengaluru\",\n",
    "  \"Kerala\": \"Thiruvananthapuram\",\n",
    "  \"Madhya Pradesh\": \"Bhopal\",\n",
    "  \"Maharashtra\": \"Mumbai\",\n",
    "  \"Manipur\": \"Imphal\",\n",
    "  \"Meghalaya\": \"Shillong\",\n",
    "  \"Mizoram\": \"Aizawl\",\n",
    "  \"Nagaland\": \"Kohima\",\n",
    "  \"Odisha\": \"Bhubaneswar\",\n",
    "  \"Punjab\": \"Chandigarh\",\n",
    "  \"Rajasthan\": \"Jaipur\",\n",
    "  \"Sikkim\": \"Gangtok\",\n",
    "  \"Tamil Nadu\": \"Chennai\",\n",
    "  \"Telangana\": \"Hyderabad\",\n",
    "  \"Tripura\": \"Agartala\",\n",
    "  \"Uttar Pradesh\": \"Lucknow\",\n",
    "  \"Uttarakhand\": \"Dehradun\",\n",
    "  \"West Bengal\": \"Kolkata\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3202dd4b-7133-4b7e-9029-98b4a47f0b57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_capital = spark.createDataFrame(capitals.items(), [\"state\", \"city\"])\n",
    "display(df_capital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dc0821f-e469-4823-908e-6ecc23ff45dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_2 = df.join(df_capital, (df.state == df_capital.state) & (df.city == df_capital.city), \"inner\")\n",
    "display(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "180d6723-1b9d-4740-af6d-9275e4e4569c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_2 = df_2.drop(df_capital.state,df_capital.city)\n",
    "display(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c754ea2f-138a-455e-af97-990a2e5274ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_2 = df_2.dropDuplicates()\n",
    "display(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64da67e3-ea87-4114-82df-ad222740fc1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_2.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"faampn6.anuradha.air_quality_silver\")\n",
    "display(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83acf998-d7a7-44f8-825a-bf3b2b7e403d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver = spark.read.table(\"faampn6.anuradha.air_quality_silver\")\n",
    "display(df_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43f051cf-7958-41fc-8264-8d57780bc5f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    " \n",
    "df_silver = df_silver.withColumn(\"date\", to_date(\"Data_Fetch_time\"))\n",
    "display(df_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3932a42-b69d-4c94-87a4-b5dd388d1024",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    " \n",
    "df_clean = df_silver.withColumn(\"pollutant_avg\", F.when(F.col('pollutant_avg') == 'NA', 0).otherwise(F.col('pollutant_avg')))\\\n",
    "    .withColumn(\"pollutant_min\", F.when(F.col('pollutant_min') == 'NA', 0).otherwise(F.col('pollutant_min')))\\\n",
    "    .withColumn(\"pollutant_max\", F.when(F.col('pollutant_max') == 'NA', 0).otherwise(F.col('pollutant_max')))\n",
    "display(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cdd1e53-d3e4-483d-9c30-c78042d61de0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df_summary = df_clean.groupBy(\"state\", \"city\", \"pollutant_id\", \"date\")\\\n",
    "    .agg(\n",
    "        F.min(\"pollutant_min\").alias(\"Min_Pollutant\"),\n",
    "        F.max(\"pollutant_max\").alias(\"Max_Pollutant\"),\n",
    "        F.avg(\"pollutant_avg\").alias(\"Avg_Pollutant\")\n",
    "        )\n",
    " \n",
    "display(df_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f75c376-6d93-480e-b710-9b6f0e480d07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_summary.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"faampn6.anuradha.air_quality_gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "330895e1-999c-4bad-b5a3-7faa2dbe84c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_gold = spark.read.table(\"faampn6.anuradha.air_quality_gold\")\n",
    "display(df_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2ce865e-2df6-4c4f-b392-ee2763b14ce8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    " \n",
    "df_pd = df_gold.select(\"Max_Pollutant\", \"Avg_Pollutant\", \"pollutant_id\", \"state\", \"city\").toPandas()\n",
    "fig = px.scatter(df_pd, x=\"Avg_Pollutant\", y=\"Max_Pollutant\", color=\"pollutant_id\", hover_data=[\"state\", \"city\"])\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Anuradha Capstone",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
